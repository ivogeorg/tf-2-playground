{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT model\n",
    "\n",
    "**Acknowledgements:**\n",
    "1. TensorFlow [Colab tutorial](https://www.tensorflow.org/official_models/fine_tuning_bert).  \n",
    "2. [Github notebook](https://github.com/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb).  \n",
    "\n",
    "**References:**\n",
    "1.  \n",
    "2.  \n",
    "\n",
    "**Table of contents:**\n",
    "1. [Setup](#Setup)  \n",
    "   1. [tf-nightly](#tf-nightly)  \n",
    "   2. [Imports](#Imports)  \n",
    "   3. [Resources](#Resources)  \n",
    "2. [Data](#Data)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### tf-nightly\n",
    "\n",
    "Basic `conda` environment:\n",
    "```\n",
    "conda install -n tf-nightly python=3.7 cudatoolkit=10.1.243\n",
    "conda install matplotlib jupyter notebook\n",
    "```\n",
    "`tf-nightly` installs `tensorflow-hub` and latest `tensorflow-datasets` among others, re-installs to the versions it needs, and installs the latest development verison of `tensorflow`. \n",
    "```\n",
    "pip install tf-nightly\n",
    "pip install tf-models-nightly\n",
    "```\n",
    "Updated tf to a dev version `2.4.0-dev20200702`. `official` seems to be part of `tf-nightly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivogeorg/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:44: UserWarning: You are currently using a nightly version of TensorFlow (2.4.0-dev20200702). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the required submodules\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Configuration, vocabulary, and pre-trained checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_config.json',\n",
       " 'bert_model.ckpt.data-00000-of-00001',\n",
       " 'bert_model.ckpt.index',\n",
       " 'vocab.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)  # TODO: How does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained BERT encoder from TF Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_url_bert = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Data to be used is from [GLUE MRPC on TFDS](https://www.tensorflow.org/datasets/catalog/glue#gluemrpc). The dataset is not set up to be directly fed into a BERT model, so some preprocessing is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset from TFDS\n",
    "\n",
    "MRPC [(Dolan & Brockett, 2005)](https://www.aclweb.org/anthology/I05-5002/) is a sentence-pair corpus automatically extracted from news sources, with human annotations for semantic equivalence (yes or no) of the sentences in each pair.\n",
    "* Labels: 2  \n",
    "* Training: 3668  \n",
    "* Evaluation: 408  \n",
    "* Max sequence length: 128  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset glue/mrpc/1.0.0 (download: 1.43 MiB, generated: Unknown size, total: 1.43 MiB) to /home/ivogeorg/tensorflow_datasets/glue/mrpc/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bc8c83443b4f71b4f78aee6ecde470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2900b1e62ce643bfaa1aab91a36b7da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivogeorg/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/home/ivogeorg/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/home/ivogeorg/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ivogeorg/tensorflow_datasets/glue/mrpc/1.0.0.incompleteEDPQTD/glue-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b223cbde9ecd4adf8c531405fc389838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3668.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ivogeorg/tensorflow_datasets/glue/mrpc/1.0.0.incompleteEDPQTD/glue-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7215559d487247d99e4aaacf0c9f0666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /home/ivogeorg/tensorflow_datasets/glue/mrpc/1.0.0.incompleteEDPQTD/glue-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0400c79440c45bfad7b123e1c550b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1725.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /home/ivogeorg/tensorflow_datasets/glue/mrpc/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Dataset is small, so load the whole\n",
    "glue, info = tfds.load('glue/mrpc', with_info=True, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'validation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glue.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info` object describes the dataset and its features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'idx': tf.int32,\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "    'sentence1': Text(shape=(), dtype=tf.string),\n",
       "    'sentence2': Text(shape=(), dtype=tf.string),\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two classes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_equivalent', 'equivalent']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx      : 1680\n",
      "label    : 0\n",
      "sentence1: b'The identical rovers will act as robotic geologists , searching for evidence of past water .'\n",
      "sentence2: b'The rovers act as robotic geologists , moving on six wheels .'\n"
     ]
    }
   ],
   "source": [
    "glue_train = glue['train']\n",
    "\n",
    "for key, value in glue_train.items():\n",
    "    print(f'{key:9s}: {value[0].numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenizer\n",
    "\n",
    "For fine-tuning a pre-trained model, exactly the same tokenization, vocabulary, and index mapping should be used as were used for training.\n",
    "\n",
    "The BERT tokenizer is written in pure Python (that is, it is not built out of TF ops), so it cannot be plugged into the model as a `keras.layer` (like, for example, [`preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization).\n",
    "\n",
    "The tokenizer has to be rebuilt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  30522\n"
     ]
    }
   ],
   "source": [
    "# Set up tokenizer to generate TF dataset\n",
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, 'vocab.txt'),\n",
    "    do_lower_case=True)\n",
    "\n",
    "print('Vocab size: ', len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'tensor', '##flow', '!']\n",
      "[7592, 23435, 12314, 999]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello TensorFlow!')\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Manual preprocessing of the data into the format expected by the model. Since the MRPC dataset it small, this can be done easily in memory. (For larger datasets, the `tf_models` library has preprocessing and dataset re-serializing tools. Details a shown in the [Appendix](#Appendix).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged tensors\n",
    "\n",
    "TF2 [tutorial on ragged tensors](https://www.tensorflow.org/guide/ragged_tensor). From the overview:\n",
    "\n",
    "*Ragged tensors* are the TensorFlow equivalent of *nested variable-length lists*. They make it easy to store and process data with *non-uniform shapes*, including:\n",
    "\n",
    "* Variable-length features, such as the set of actors in a movie.  \n",
    "* Batches of variable-length sequential inputs, such as sentences or video clips.  \n",
    "* Hierarchical inputs, such as text documents that are subdivided into sections, paragraphs, sentences, and words.  \n",
    "* Individual fields in structured inputs, such as protocol buffers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence encoding\n",
    "\n",
    "The model expects the two sentences from the pair to *concatenated* together. The input should start with the *classification-problem token* `[CLS]` and end with the *separator token* `[SEP]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 102]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all the sentences, appending a `[SEP]` token and packing them into [*ragged tensors*](#Ragged-tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s):\n",
    "    tokens = list(tokenizer.tokenize(s.numpy()))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "sentence1 = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in glue_train['sentence1']\n",
    "])\n",
    "sentence2 = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in glue_train['sentence2']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1 shape:  [3668, None]\n",
      "Sentence2 shape:  [3668, None]\n"
     ]
    }
   ],
   "source": [
    "print('Sentence1 shape: ', sentence1.shape.as_list())\n",
    "print('Sentence2 shape: ', sentence2.shape.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepend the `[CLS]` token and concatenate the ragged tensors to form a single `input_word_ids` tensor for each example. [`RaggedTensor.to_tensor()`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#to_tensor) zero-pads to the longest sequence (in this case, 128, which is a parameter of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
