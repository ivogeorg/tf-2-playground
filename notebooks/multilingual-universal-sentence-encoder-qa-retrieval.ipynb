{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Universal Sentence Encoder Q&A Retrieval\n",
    "\n",
    "**Acknowledgements:**\n",
    "1. Tutorial on [Colab](https://www.tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa)  \n",
    "2. Notebook on [Github](https://github.com/tensorflow/hub/blob/master/examples/colab/retrieval_with_tf_hub_universal_encoder_qa.ipynb)  \n",
    "\n",
    "**References:**  \n",
    "1. [Universal Encoder Multilinqual Q&A Model](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3)  \n",
    "2. SQuAD dataset:\n",
    "   1. [Home](https://rajpurkar.github.io/SQuAD-explorer/).  \n",
    "   2. [Retrieval evaluation](https://github.com/google/retrieval-qa-eval)  \n",
    "   3. [v1.0 paper](https://arxiv.org/abs/1606.05250)  \n",
    "   4. [v2.0 paper](https://arxiv.org/abs/1806.03822)  \n",
    "3. Simple Neighbors library:  \n",
    "   1. [Docs](https://simpleneighbors.readthedocs.io/en/latest/)  \n",
    "   2. [pypi](https://pypi.org/project/simpleneighbors/)  \n",
    "4. TensorFlow components:\n",
    "   1. [TF Text (Github guide w/ examples](https://github.com/tensorflow/text)  \n",
    "   2. [TF Embeddings (tutorial)](https://www.tensorflow.org/tutorials/text/word_embeddings)  \n",
    "5. NLTK:\n",
    "   1. [Home](https://www.nltk.org/)  \n",
    "   2. [Book](https://www.nltk.org/book/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of Contents:**  \n",
    "1. [Example use of the sentence encoder model](#Example-use)  \n",
    "2. [Tutorial setup](#Setup)  \n",
    "3. [SQuAD utiltity functions](#SQuAD-utility-functions)  \n",
    "4. [Visualization functions](#Visualization-functions)  \n",
    "5. [SQuAD extraction](#SQuAD-extraction)  \n",
    "6. [Encoder setup](#Encoder-setup)  \n",
    "7. [Embedding computation](#Embedding-computation) with **`response_encoder`**\n",
    "8. [Retrieval](#Retrieval) with **`question_encoder`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use\n",
    "\n",
    "Example of using the [universal-sentence-encoder-multilingual-qa](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3) model. Doesn't do much but serves as a canary for the environment. Computes the dot product of the question and response embeddings to identify the most likely response.\n",
    "\n",
    "### TF Hub module signatures\n",
    "*Signatures* are [input-output specifications for TF Hub modules](https://www.tensorflow.org/hub/common_signatures), aiming to achieve interoperability and interchangeability without knowing the internals. For the sentence encoder, they are `question_encoder` and `response_encoder`. Notice they are called as follows:\n",
    "```python\n",
    "module.signatures['question_encoder']()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4088399 , 0.08877401]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "\n",
    "questions = [\"What is your age?\"]\n",
    "responses = [\"I am 20 years old.\", \"good morning\"]\n",
    "response_contexts = [\"I will be 21 next year.\", \"great day.\"]\n",
    "\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "\n",
    "question_embeddings = module.signatures['question_encoder'](\n",
    "            tf.constant(questions))\n",
    "response_embeddings = module.signatures['response_encoder'](\n",
    "        input=tf.constant(responses),\n",
    "        context=tf.constant(response_contexts))\n",
    "\n",
    "np.inner(question_embeddings['outputs'], response_embeddings['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ivogeorg/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import simpleneighbors\n",
    "import urllib\n",
    "from IPython.display import HTML, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQuAD utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_squad(url):\n",
    "    return json.load(urllib.request.urlopen(url))\n",
    "\n",
    "def extract_sentences_from_squad_json(squad):\n",
    "    all_sentences = []\n",
    "    for data in squad['data']:\n",
    "        for paragraph in data['paragraphs']:\n",
    "            sentences = nltk.tokenize.sent_tokenize(paragraph['context'])\n",
    "            all_sentences.extend(zip(sentences, [paragraph['context']] * len(sentences)))  # (text, context) where context is all the sentences\n",
    "    return list(set(all_sentences))  # remove duplicates\n",
    "\n",
    "def extract_questions_from_squad_json(squad):\n",
    "    questions = []\n",
    "    for data in squad['data']:\n",
    "        for paragraph in data['paragraphs']:\n",
    "            for qas in paragraph['qas']:\n",
    "                if qas['answers']:\n",
    "                    questions.append((qas['question'], qas['answers'][0]['text']))\n",
    "    return list(set(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_with_highlight(text, highlight):\n",
    "    output = '<li> '\n",
    "    i = text.find(highlight)\n",
    "    while True:\n",
    "        if i == -1:\n",
    "            output += text\n",
    "            break\n",
    "        output += text[0:i]\n",
    "        output += '<b>' + text[i:i + len(highlight)] + '<b>'\n",
    "        text = text[i + len(highlight):]\n",
    "        i = text.find(highlight)\n",
    "    return output + '</li>\\n'\n",
    "\n",
    "def display_nearest_neighbors(query_text, answer_text=None):\n",
    "    query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n",
    "    search_results = index.nearest(query_embedding, n=num_results)\n",
    "    \n",
    "    if answer_text:\n",
    "        result_md = '''\n",
    "        <p>Random Question from SQuAD:</p>\n",
    "        <p>&nbsp;&nbsp;<b>%s</b></p>\n",
    "        <p>Answer:</p>\n",
    "        <p>&nbsp;&nbsp;<b>%s</b></p>\n",
    "        ''' % (query_text, answer_text)\n",
    "    else:\n",
    "        result_md = '''\n",
    "        <p>Random Question from SQuAD:</p>\n",
    "        <p>&nbsp;&nbsp;<b>%s</b></p>\n",
    "        ''' % query_text\n",
    "        \n",
    "    result_md += '''\n",
    "        <p>Retrieved sentences:\n",
    "        <ol>\n",
    "        '''\n",
    "    \n",
    "    if answer_text:\n",
    "        for s in search_results:\n",
    "            result_md += output_with_highlight(s, answer_text)\n",
    "    else:\n",
    "        for s in search_results:\n",
    "            result_md += '<li>' + s + '</li>\\n'\n",
    "            \n",
    "    result_md += '</ol>'\n",
    "    display(HTML(result_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQuAD extraction\n",
    "\n",
    "The SQuAD dataset will be extracted into:\n",
    "* **sentences** as a list of *(text, context)* tuples (each SQuAD paragraph is split into sentences and the sentence and paragraph form the *(text, context)* tuple.  \n",
    "* **questions** as a list of *(question, answer)* tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10455 sentences, 10552 questions extracted from SQuAD https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
      "\n",
      "Example sentence and context: \n",
      "\n",
      "sentence:\n",
      "\n",
      "('In GR, gravitation is not viewed as a force, but rather, objects moving '\n",
      " 'freely in gravitational fields travel under their own inertia in straight '\n",
      " 'lines through curved space-time – defined as the shortest space-time path '\n",
      " 'between two space-time events.')\n",
      "context:\n",
      "\n",
      "('Since then, and so far, general relativity has been acknowledged as the '\n",
      " 'theory that best explains gravity. In GR, gravitation is not viewed as a '\n",
      " 'force, but rather, objects moving freely in gravitational fields travel '\n",
      " 'under their own inertia in straight lines through curved space-time – '\n",
      " 'defined as the shortest space-time path between two space-time events. From '\n",
      " 'the perspective of the object, all motion occurs as if there were no '\n",
      " 'gravitation whatsoever. It is only when observing the motion in a global '\n",
      " 'sense that the curvature of space-time can be observed and the force is '\n",
      " \"inferred from the object's curved path. Thus, the straight line path in \"\n",
      " 'space-time is seen as a curved line in space, and it is called the ballistic '\n",
      " 'trajectory of the object. For example, a basketball thrown from the ground '\n",
      " 'moves in a parabola, as it is in a uniform gravitational field. Its '\n",
      " 'space-time trajectory (when the extra ct dimension is added) is almost a '\n",
      " 'straight line, slightly curved (with the radius of curvature of the order of '\n",
      " 'few light-years). The time derivative of the changing momentum of the object '\n",
      " 'is what we label as \"gravitational force\".')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "squad_versions = [\n",
    "    \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\", \n",
    "    \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\", \n",
    "    \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\",\n",
    "    \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"]\n",
    "squad_url = squad_versions[3]\n",
    "\n",
    "squad_json = download_squad(squad_url)\n",
    "sentences = extract_sentences_from_squad_json(squad_json)\n",
    "questions = extract_questions_from_squad_json(squad_json)\n",
    "print('{} sentences, {} questions extracted from SQuAD {}'.format(len(sentences), len(questions), squad_url))\n",
    "\n",
    "print('\\nExample sentence and context: \\n')\n",
    "sentence = random.choice(sentences)\n",
    "print('sentence:\\n')\n",
    "pprint.pprint(sentence[0])\n",
    "print('context:\\n')\n",
    "pprint.pprint(sentence[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3'\n",
    "model = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding computation\n",
    "\n",
    "The embedding of all the (text, context) tuples are computed and stored in a [`simpleneighbors`](https://pypi.org/project/simpleneighbors/) index using the **`response_encoder`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embedding for 10455 sentences\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d20d06505dc469db93afda5d3dd3677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=104.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "simpleneighbors index for 10455 sentences built.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "encodings = model.signatures['response_encoder'](\n",
    "    input=tf.constant([sentences[0][0]]),\n",
    "    context=tf.constant([sentences[0][1]])\n",
    ")\n",
    "\n",
    "index = simpleneighbors.SimpleNeighbors(\n",
    "    len(encodings['outputs'][0]), metric='angular')\n",
    "\n",
    "print('Computing embedding for {} sentences'.format(len(sentences)))\n",
    "\n",
    "slices = zip(*(iter(sentences),) * batch_size)  # TODO: Parse these two lines\n",
    "num_batches = int(len(sentences) / batch_size)\n",
    "\n",
    "for s in tqdm(slices, total=num_batches):\n",
    "    response_batch = list([r for r, c, in s])\n",
    "    context_batch = list([c for r, c in s])\n",
    "    encodings = model.signatures['response_encoder'](\n",
    "        input=tf.constant(response_batch),\n",
    "        context=tf.constant(context_batch)\n",
    "    )\n",
    "    for batch_index, batch in enumerate(response_batch):\n",
    "        index.add_one(batch, encodings['outputs'][batch_index])\n",
    "        \n",
    "index.build()\n",
    "\n",
    "print('simpleneighbors index for {} sentences built.'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Upon retrieval, the question is encoded using the **`question_encoder`** the question embedding is used to query the [`simpleneighbors`](https://pypi.org/project/simpleneighbors/) index. **TODO:** Split the question encoding out of the neighbor display in `display_nearest_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <p>Random Question from SQuAD:</p>\n",
       "        <p>&nbsp;&nbsp;<b>In 1890, who did the university decide to team up with?</b></p>\n",
       "        <p>Answer:</p>\n",
       "        <p>&nbsp;&nbsp;<b>several regional colleges and universities</b></p>\n",
       "        \n",
       "        <p>Retrieved sentences:\n",
       "        <ol>\n",
       "        <li> The Royal University of Warsaw was established in 1816.</li>\n",
       "<li> However, the university chose to withdraw from the conference in 1946 after University President Robert Maynard Hutchins de-emphasized varsity athletics in 1939 and dropped football.</li>\n",
       "<li> Some members of this community emigrated to the United States in the 1890s.</li>\n",
       "<li> Harvard's graduate schools, which had accepted females and other groups in greater numbers even before the college, also became more diverse in the post-World War II period.</li>\n",
       "<li> In 1685, Rev.</li>\n",
       "<li> By 1971, the decision was made to also cancel missions 18 and 19.</li>\n",
       "<li> The University of Warsaw was established in 1816, when the partitions of Poland separated Warsaw from the oldest and most influential Polish academic center, in Kraków.</li>\n",
       "<li> After the 1940s, the Gothic style on campus began to give way to modern styles.</li>\n",
       "<li> Despite that the Warsaw University of Technology building (1899–1902) is the most interesting of the late 19th-century architecture.</li>\n",
       "<li> Construction took place between 1899 and 1909.</li>\n",
       "<li> The charter creating the Harvard Corporation was granted in 1650.</li>\n",
       "<li> In accordance with the decisions of the Congress of Vienna, the Russian Empire annexed Warsaw in 1815 and it became part of the \"Congress Kingdom\".</li>\n",
       "<li> The population of Pons Aelius at this period was estimated at 2,000.</li>\n",
       "<li> At the time, they constituted the majority of the townspeople.</li>\n",
       "<li> The color was unofficially adopted (in preference to magenta) by an 1875 vote of the student body, although the association with some form of red can be traced back to 1858, when Charles William Eliot, a young graduate student who would later become Harvard's 21st and longest-serving president (1869–1909), bought red bandanas for his crew so they could more easily be distinguished by spectators at a regatta.</li>\n",
       "<li> Harvard was formed in 1636 by vote of the Great and General Court of the Massachusetts Bay Colony.</li>\n",
       "<li> In 1638, the college became home for North America's first known printing press, carried by the ship John of London.</li>\n",
       "<li> Newcastle was successfully defended against the Scots three times during the 14th century, and was created a county corporate with its own sheriff by Henry IV in 1400.</li>\n",
       "<li> The largest portion of the Huguenots to settle in the Cape arrived between 1688 and 1689 in seven ships as part of the organised migration, but quite a few arrived as late as 1700; thereafter, the numbers declined and only small groups arrived at a time.</li>\n",
       "<li> Incorporated first by Henry II, the city had a new charter granted by Elizabeth in 1589.</li>\n",
       "<li> Several congregations were founded, such as those of Fredericia (Denmark), Berlin, Stockholm, Hamburg, Frankfurt, Helsinki, and Emden.</li>\n",
       "<li> Leopold Kronenberg Palace).</li>\n",
       "<li> In 1825 George Stephenson built the Locomotion for the Stockton and Darlington Railway.</li>\n",
       "<li> After Spain ceded the Florida Territory to the United States in 1821, American settlers on the north side of the Cow Ford decided to plan a town, laying out the streets and plats.</li>\n",
       "<li> Led by Isaiah D. Hart, residents wrote a charter for a town government, which was approved by the Florida Legislative Council on February 9, 1832.</li>\n",
       "</ol>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_results = 25\n",
    "\n",
    "query = random.choice(questions)\n",
    "display_nearest_neighbors(query[0], query[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "1. Review SQuAD: question, answer, text, context. *How is the answer supposed to be generated/retrieved?*  \n",
    "2. What do the points in the `SimpleNeighbor` index represent? How is it an *index*?  \n",
    "3. Are only the senteces indexed? Isn't this tantamount to *\"taking them out of context\"*?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
